[
  {
    "title": "Data-Driven Gym: Scraping and Analysing Gym Attendance with Prometheus",
    "url": "/blog/gym/",
    "categories": "Project",
    "tags": "Grafana, Prometheus, Python",
    "date": "2025-04-04 12:00:00 +0930",
    "snippet": "IntroductionIf you’re anything like me, you tend to be a bit curious about how things work behind the scenes. Maybe it’s a slight hacker tendency or just a habit of not taking data at face value. S...",
    "content": "IntroductionIf you’re anything like me, you tend to be a bit curious about how things work behind the scenes. Maybe it’s a slight hacker tendency or just a habit of not taking data at face value. So when I saw the digital attendance counters on the REDACTED Fitness application, I couldn’t help but wonder how accurate they really were.I built a Prometheus exporter that scrapes attendance data from all REDACTED Fitness locations. What started as a simple attempt to verify if the counters ever hit zero quickly transformed into a comprehensive data gathering initiative. The project now lives on my home server, integrated with my existing homelab infrastructure, collecting valuable insights about gym attendance patterns across different locations.In this post, I’ll walk you through how this project came to be, what I discovered, and how you might benefit from similar data about your own gym.The Problem &amp; HypothesisMy scepticism began with a simple observation, people were being counted when entering the gym, but what about when they left? I suspected various scenarios, such as tailgating (when multiple people exit but only one scan occurs) might be causing inaccuracies in the reported numbers.My primary hypothesis was straightforward, if the counting system was accurate, we should see the numbers hit zero at some point, likely during overnight hours when the gym is empty. If the numbers never reached zero, it would suggest a cumulative error in the system.This might seem trivial, but accurate attendance data is valuable for both gym-goers (who want to avoid crowds) and gym operators (who need to understand usage patterns for staffing and expansion decisions).Finding the DataWhilst the website lists the live tracker functionality all over it, all the links direct you to download the app. This is fair, this gym is unlike most traditional gyms with lock-in contracts and franchises. REDACTED instead has their app that they strongly advertise, and no doubt a lot of the company’s valuation would be based around the application they have to track all aspects of their members. This left me to try and reverse engineer how to find the count.Based on the page HTML structure, it was evident that the REDACTED website is made in WordPress, so I started to look in all of the usual places. I found the sitemap, but after looking through it, there were no links to the live counter.My investigative instincts kicked in. I then looked at other common locations where hidden resources might be lurking. Maybe if they were hiding it from the sitemap, they’d have it in the robots.txt file to prevent scrapers from finding it. No luck there either.Before I went full nuclear and started brute-forcing likely URLs with a tool like GObuster, I had an idea. Just because the URL wasn’t listed in the current sitemap didn’t mean it hadn’t always been there. I wrote a quick and dirty script to search through public archives from the Wayback Machine, specifically looking for any sites with the string “live” in the URL.Thankfully this script yielded two results. Even better, one of them happened to be exactly the page I was looking for. When I accessed the page, I was pleasantly surprised. The returned HTML was quite nice well-structured and formatted in a way that made scraping straightforward, such as &lt;span data-live-count=\"Elliot\" class=\" relative text-4xl text-black font-bold\"&gt;64&lt;/span&gt;.Page StrucutreEach gym’s live count is represented by a span element. The data-live-count attribute contains the name of the gym (e.g., \"Elliot\"), and the actual count is stored as the inner text of that element (e.g., 64).The live counter website includes a dropdown to choose your gym, but thankfully it has all of the gym counts included in the HTML (the elements are just hidden until you choose that gym). Using this structure, I could use BeautifulSoup (BS4) to scrape one page and create a dictionary with all of the counts. This discovery meant I didn’t need to send multiple complex requests or implement intricate scraping logic. The data was right there, just waiting to be extracted.There are a few quirks in how the counts are shown. Most notably, when a gym has zero occupants the span doesn’t display a 0. Instead, it appears completely blank. This may be a deliberate decision to avoid displaying potentially misleading or negative numbers. For example, if the entry and exit systems become unsynchronised due to tailgating as discussed previously, the displayed count might drift below zero. Hiding the count when it falls below one could be a simple workaround to prevent this from being shown publicly.Technical ImplementationBuilding the exporter was relatively straightforward. I created a Python script that:  Makes requests to the REDACTED Fitness website at regular intervals  Parses the HTML to extract the current attendance numbers for each location  Transforms this data into Prometheus metrics  Exposes these metrics via an HTTP endpoint that Prometheus can scrapeFor the backend infrastructure, I leveraged my existing Prometheus and Grafana setup. Prometheus handles the data collection and storage, while Grafana provides visualization capabilities with customisable dashboards. This allowed me to add the gym metrics alongside my existing homelab monitoring with minimal additional configuration.Project EvolutionWhen February rolled around, the project took on new significance. The annual influx of “New Year’s resolution warriors” provided a natural experiment in gym attendance patterns. I watched with interest as the numbers swelled in early January, peaked in mid-January, and then gradually declined through February.This phenomenon transformed my project from a simple verification tool into a longitudinal study. I began tracking retention rates, comparing month on moth data, and monitoring how different locations handled the seasonal surge.But I ran into a snag I didn’t catch until it was too late. My Prometheus container was set up with a short data retention window, because originally it was just logging short-term metrics like Minecraft server performance or CPU usage. In the Docker config for prometheus-docker, I had the retention set like this:Retention Time: 90dThat meant any data older than 90 days was automatically wiped. I didn’t realise what was happening, and by the time I caught it (about two months later) I’d already lost all the January and February data. That included the entire New Year’s resolution spike, which was the whole point of tracking it in the first place.I’ve sorted it now. I bumped the retention up to 10 years:Retention Time: 10yI’ve got the disk space for it, so may as well keep the data. Now everything gets logged long-term, and I won’t get caught out again when I actually want to compare things across years.Practical ApplicationsBeyond satisfying my curiosity, this project has delivered tangible benefits:For me personally, I can now identify the optimal times to visit each REDACTED location. For example, I discovered that my local gym is consistently far busier on Mondays than any other day of the week and as such I now have that as a permanent rest day in my routine. Tuesdays, are also quite busy so I usually aim to do legs on Tuesday as the leg machines are often less in demand than other machines.The location-specific insights are particularly valuable. Each gym has unique patterns based on its location, demographics, and facilities. The CBD locations show clear business-hour patterns, while suburban locations have more consistent attendance throughout the day with peaks after work hours.I’ve even begun to recognise patterns related to weather, attendance drops significantly during extremely hot days and spikes when outdoor exercise would be uncomfortable.These insights help me plan my workouts efficiently, avoiding peak times when equipment availability might be limited.Findings &amp; InsightsThe first major revelation? My hypothesis was correct-the numbers never hit zero, even during the middle of the night when the gym was certainly empty. This confirmed my suspicion about counting inaccuracies.But the data revealed much more interesting patterns beyond this initial finding:  Each location has distinct “personality traits” in terms of attendance patterns  Monday evenings are consistently the busiest across all locations  Saturday afternoons are surprisingly quiet at most locations  There’s a noticeable drop in attendance during public holidays  Some locations see sharp spikes during lunch hours on weekdays, while others have a more gradual distribution  Gyms tend to be busiest at the start of the week, especially on Mondays, and gradually taper off as the week progressesThis downward trend is one of the clearest patterns across nearly every location. Mondays often flirt with each gym’s all-time highs, while by Thursday or Friday things are much quieter. You can see this clearly in the screenshot below, but it’s even more apparent in the second example in this section, where the view is narrowed to a single month. With the shorter timeframe, the weekly rhythm stands out much more clearly.Week-on-week gym attendance trends over a three-month period. The steady drop-off from Monday through to the weekend.Detecting and Diagnosing AnomaliesThe data also serves as an effective monitoring system for identifying operational issues. By calculating the rate of change (derivative) of attendance numbers across locations, as well as comparing week-on-week trends and differences between locations, anomalies become immediately apparent. Over the past few months, I’ve observed distinct categories of anomalies in the data:1. The OverrunAn overrun happens when the live count at a gym suddenly spikes well beyond what would normally be possible. This type of error has surprisingly occurred a few times.The most notable example was on the 2nd of March, when all locations outside of Western Australia began counting up rapidly and eventually exceeded their all-time highs by a significant margin. Then, at around 12:30 PM local time, every gym abruptly snapped back to a more typical number.This kind of behaviour suggests something went wrong in the counting backend. The sharp reset back to expected values makes it likely the issue was identified and corrected mid-day.You can also see a more subtle example of this in the same dataset on the 7th of March, where there’s a short-lived but noticeable spike. These events aren’t isolated either, the same extreme overrun pattern happened again on the 2nd of July 2025, where many locations reported multiple times their usual counts around 6 PM. Since this occurred during peak gym hours, the inflated values appeared much more dramatic than the earlier midday examples, making the anomaly far more obvious in the data.All locations outside of Western Australia over runnning by up to multiple times their previous highest, before sharp correction2. When the Data Goes DarkThe second type of anomaly is when check-ins stop entirely. While it’s normal to see no new entries around 4 AM, seeing this happen during the middle of the day without explanation is a strong signal that something has gone wrong. If a location usually has a steady pattern of activity and that suddenly stops, it often points to a scanner failure.Short interruptions can sometimes be caused by technical glitches, but longer periods with no data are more commonly due to temporary closures. Still, if the downtime happens during peak hours and with no notice, it’s always worth investigating.In the example below, one location’s count stopped updating altogether and remained blank for an extended period. It stood out clearly in the data. A quick search of the gym revealed an official announcement confirming the reason:  We are so excited to announce that REDACTED Fitness Cranbourne is getting a makeover!  To get to work, our Cranbourne gym will be closed from 8:00am Monday 28/04/2025 and we will reopen at 7:00am Friday 20/06/2025.This kind of planned closure is rare but detectable. The key sign is a complete lack of check-in activity over many days.3. Anomalous LowsThese types of errors are typically caused by on-site issues at a specific location. In most cases, it’s due to a malfunctioning scanner. From what I’ve seen, this isn’t always a complete failure. Sometimes the scanner is still technically working, but only picks up one in every 20 check-in attempts.When that happens, most gym goers won’t hang around and brute force the scanner with check-in attempts. They’ll either tailgate someone else through the gate or use an alternative entry method if available. As a result, the official check-in count ends up being much lower than it should be.You can spot these anomalies by comparing that gym’s attendance trend against others. Since most REDACTED locations follow fairly consistent patterns throughout the day, a significant drop at one site stands out quickly. Another good way to detect it is by comparing week-on-week attendance. If a location suddenly shows far fewer entries on a day that normally sees heavy traffic, there’s a good chance something’s broken.Example data from the Parafield location showing anomalously low attendance on the 20th and 30th of March4. Non-erroneous AnomaliesNot all anomalies indicate something is broken. Sometimes, they reveal useful patterns hiding in plain sight. One of the most obvious examples is opening day for a new gym. These days almost always show the highest rate of change ever recorded for that location.It makes sense, there’s a buzz, people are curious, and there’s often a launch event drawing a crowd. The result is a sharp and sudden surge in check-ins, far beyond what you’d expect on a normal day.Unlike scanner faults or data outages, these spikes aren’t errors. But they stand out clearly when visualised alongside more typical daily trends, and they’re a great example of how rate-of-change metrics can surface significant real-world events.Technical Details &amp; Open SourceThe entire project is available on GitHub:github.com/TravisPooley/RevoFitness-Attendance-ExporterThe core components include:  A Python-based Prometheus exporter using BeautifulSoup for HTML parsing  Docker configuration for easy deployment  Grafana dashboard templates for immediate visualizationInstallation is straightforward for anyone familiar with Docker and Prometheus. The repository includes detailed documentation on getting started, configuring the scraper for different gym chains (the code is adaptable beyond just REDACTED), and setting up alerting for specific thresholds.I’ve designed the system to be lightweight, consuming minimal resources on even the most modest homelab setups. The entire stack can run comfortably on a Raspberry Pi or any small server.Technical Implementation Deep DiveBuilding the exporter required solving several technical challenges: reliable data extraction, proper Prometheus integration and robust error handling. Let me walk you through the key components that make this system work.The Core Scraping LogicThe heart of the system is a Python-based collector that implements Prometheus’s custom collector interface. Rather than pre-collecting data at regular intervals, this approach scrapes fresh data every time Prometheus requests metrics, ensuring we never serve stale data.class RevoFitnessGymCollector:    \"\"\"Collector to scrape Revo Fitness gym counts at scrape time.\"\"\"    def __init__(self, target_url, timeout=10):        self.target_url = target_url        self.timeout = timeout        # Metrics for monitoring the scraper itself        self.scrape_errors = Counter(            'revo_fitness_scrape_errors_total',            'Total number of scraping errors',            ['error_type']        )        self.scrape_success = Counter(            'revo_fitness_scrape_success_total',            'Total number of successful scrapes'        )        self.scrape_duration = Histogram(            'revo_fitness_scrape_duration_seconds',            'Time spent scraping the Revo Fitness website'        )    def collect(self):        \"\"\"Dynamically collect metrics on each Prometheus scrape.\"\"\"        metric = GaugeMetricFamily(            'revo_fitness_gym_count',            'Live gym occupancy count',            labels=['gym_name']        )        with self.scrape_duration.time():            try:                response = requests.get(                    self.target_url,                    timeout=self.timeout,                    headers={'User-Agent': 'Prometheus-Exporter-RevoFitness/1.0'}                )                response.raise_for_status()                soup = BeautifulSoup(response.text, 'html.parser')                count_data = {}                # Extract gym counts from span elements                for span in soup.find_all('span', attrs={'data-live-count': True}):                    gym_name = span['data-live-count'].strip()                    try:                        count = int(span.text.strip())                    except ValueError:                        count = -1  # Invalid data marker                    count_data[gym_name] = count                # Add metrics for each gym                for gym_name, count in count_data.items():                    metric.add_metric([gym_name], count)                self.scrape_success.inc()                logger.info(f\"Successfully scraped {len(count_data)} gyms.\")            except requests.RequestException as e:                self.scrape_errors.labels(error_type=type(e).__name__).inc()                logger.error(f\"Request error: {e}\")            except Exception as e:                self.scrape_errors.labels(error_type=type(e).__name__).inc()                logger.error(f\"Unexpected error: {e}\")        yield metricThe key insight here is using BeautifulSoup to parse the HTML and extract data from elements with the data-live-count attribute. This approach is resilient to minor HTML changes and provides clean separation between gym names and their counts.Prometheus IntegrationI tried to build the exporter so it follows Prometheus best practices by implementing a custom collector rather than using static metrics. This ensures that each scrape gets fresh data and that we can monitor the scraper’s own performance:  Scrape duration: Tracks how long each scrape takes  Success/error counters: Monitor scraping reliability  Error type labels: categorise different failure modesThe main application loop is intentionally simple:def main():    parser = argparse.ArgumentParser(description='Revo Fitness Gym Count Prometheus Exporter')    parser.add_argument('--port', type=int, default=8000, help='Port to serve metrics on')    parser.add_argument('--timeout', type=int, default=10, help='HTTP request timeout in seconds')    parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], default='INFO')    args = parser.parse_args()    target_url = \"https://revofitness.com.au/livemembercount/\"    collector = RevoFitnessGymCollector(target_url, args.timeout)    REGISTRY.register(collector)    start_http_server(args.port)    logger.info(f\"Exporter running on port {args.port}\")    try:        while True:            time.sleep(1)    except KeyboardInterrupt:        logger.info(\"Exporter stopped.\")Containerisation for ReliabilityTo ensure the exporter runs consistently across different environments, I’ve containerised it using Docker. The Dockerfile is deliberately minimal:FROM python:3.13-slimWORKDIR /appCOPY requirements.txt .RUN pip install --no-cache-dir -r requirements.txtCOPY revo_fitness_exporter.py .EXPOSE 8920CMD [\"python\", \"revo_fitness_exporter.py\", \"--port\", \"8920\", \"--timeout\", \"50\"]This approach provides several benefits:  Consistency: The same environment regardless of host system  Isolation: Dependencies don’t conflict with other applications  Easy deployment: Single command to get everything running  Resource efficiency: Python slim image keeps the container smallError Handling and ResilienceThe scraper includes comprehensive error handling for common failure scenarios:Network Issues: Timeouts and connection errors are caught and logged, but don’t crash the exporter. Prometheus will simply get no new data points for that scrape interval.Parsing Errors: If the HTML structure changes or contains unexpected data, the scraper gracefully handles it by setting count values to -1, making it easy to identify data quality issues in Grafana.Rate Limiting: The user-agent header helps identify the scraper as a monitoring tool rather than a bot, hopefully reducing the likelihood of being blocked.Deployment ConfigurationIn my homelab setup, the exporter runs as a Docker container with these key configurations, the Prometheus configuration is equally straightforward:# prometheus.yml excerptscrape_configs:  - job_name: 'revo-fitness'    static_configs:      - targets: ['revo-fitness-exporter:8920']    scrape_interval: 5m    scrape_timeout: 30sPerformance ConsiderationsThe exporter is designed to be lightweight and efficient:  Minimal dependencies: Only requests, BeautifulSoup, and prometheus_client  Low memory footprint: No data caching or complex state management  Fast scraping: Typically completes in under 2 seconds  Configurable timeouts: Prevents hanging requests from affecting PrometheusThe 5-minute scrape interval provides good temporal resolution for attendance patterns while being respectful of REDACTED’s servers. For detecting rapid anomalies like evacuations, this frequency is sufficient since such events typically unfold over several minutes.Data Quality and ValidationOne crucial aspect of the implementation is data validation. The system handles several edge cases:  Missing gyms: If a location disappears from the HTML (like during closures), it simply stops reporting metrics  Invalid counts: Non-numeric values are converted to -1 for easy identification  Network failures: Temporary outages don’t crash the exporterThis robust error handling ensures that temporary issues don’t corrupt the historical data or bring down the monitoring system.ConclusionWhat began as a sceptical investigation into gym counter accuracy has evolved into a fascinating data project. Along the way, I’ve gained valuable insights into gym attendance patterns, honed my data visualisation skills, and created a useful tool for optimising my workout schedule.If you’re interested in gym attendance patterns or looking to build similar data collection tools, I encourage you to check out the repository, try it yourself. Sometimes, the most interesting projects come from questioning the accuracy of the everyday systems we take for granted. In this case, a simple doubt about gym attendance counters led to a project that continues to provide valuable insights many months later."
  },
  {
    "title": "Never Gonna Give Up Testing: Building an RTSP Docker Container for Security Camera Automation",
    "url": "/blog/rtsp/",
    "categories": "Project",
    "tags": "RTSP, FFMPEG, Docker",
    "date": "2025-04-04 12:00:00 +0930",
    "snippet": "IntroductionIn the world of home automation and security systems, testing can quickly become tedious. My recent project involving facial recognition with security cameras highlighted this problem p...",
    "content": "IntroductionIn the world of home automation and security systems, testing can quickly become tedious. My recent project involving facial recognition with security cameras highlighted this problem perfectly. After the tenth time running in and out of my camera’s field of view to trigger the system, I knew there had to be a better way to test these setups.Inspiration struck from an unexpected place - those classic bank heist movies where criminals hack security cameras to loop old footage. Surely, I thought, someone must have created a solution for this already. To my surprise, I found very little that matched what I needed. So I built it myself: a Docker container that hosts an RTSP server capable of streaming test footage to my security system.The ProblemTesting security camera automations traditionally requires:  Physically triggering the camera multiple times  Waiting for processing to complete  Adjusting settings and repeating the processThis is not only time-consuming but also impractical for fine-tuning facial recognition systems that need consistent input.The SolutionMy solution is a simple yet effective Docker container that:  Runs an RTSP server (MediaMTX)  Streams video content on demand  Can be easily integrated into existing security camera setups for testingThe beauty of this approach is that it lets you feed consistent test footage into your system without having to physically trigger your cameras repeatedly.Technical ImplementationThe implementation consists of two main components:1. The Dockerfile# Use a lightweight Debian base image to keep the image size smallFROM debian:bookworm-slim as builder# Install FFmpeg and clean up to reduce image sizeRUN apt-get update &amp;&amp; \\    apt-get install -y ffmpeg &amp;&amp; \\    apt-get clean &amp;&amp; \\    rm -rf /var/lib/apt/lists/*# Use the MediaMTX base image that includes FFmpeg supportFROM bluenviron/mediamtx:latest-ffmpeg# Copy the video file into the containerCOPY videos/rickroll.mp4 /videos/rickroll.mp4# Configure MediaMTX to use TCP only for RTSP (more firewall-friendly)ENV MTX_PROTOCOLS=tcp# Expose RTSP portEXPOSE 8554# Add and make the custom entrypoint executableCOPY entrypoint.sh /entrypoint.shRUN chmod +x /entrypoint.sh# Set the entrypoint script to launch MediaMTXENTRYPOINT [\"/entrypoint.sh\"]The Dockerfile uses a multi-stage build approach:  First stage: Uses a light weight Debian to install FFmpeg, which will handle video conversion and streaming  Second stage: Uses the MediaMTX image as the base (MediaMTX is a lightweight, ready-to-use RTSP/RTMP/HLS server)  In my exaple I only expoes the port required for RTSP but there is various protocols supported by MediaMTX which you can chose to export (8554 1935 8888 8889 8890/udp 8189/udp)  An entrypoint script handles the startup logic2. The Entrypoint Script#!/bin/sh# Start MediaMTX (RTSP server) in the background# MediaMTX provides Real-Time Streaming Protocol functionality/mediamtx &amp;# Wait for MediaMTX to fully initialize# This delay ensures the RTSP server is ready to accept connectionssleep 5# Start FFmpeg to stream video to the RTSP server# Parameters:#   -re                 = Read input at native frame rate (real-time)#   -stream_loop -1     = Loop the input file indefinitely#   -i                  = Input file path#   -c:v libx264        = Use H.264 video codec#   -preset ultrafast   = Use fastest encoding preset (reduces quality but lowers latency)#   -tune zerolatency   = Optimize for minimal latency#   -c:a copy           = Copy audio stream without re-encoding#   -f rtsp             = Output format is RTSP#   -rtsp_transport tcp = Use TCP (more reliable than UDP) for RTSP transportffmpeg -re -stream_loop -1 -i /videos/rickroll.mp4 -c:v libx264 -preset ultrafast -tune zerolatency -c:a copy -f rtsp -rtsp_transport tcp rtsp://localhost:8554/mystreamThis script:  Launches the MediaMTX server in the background  Waits for it to initialize  Uses FFmpeg to continuously stream the provided video file to the RTSP server  The -stream_loop -1 flag ensures the video loops infinitely  Video is encoded with H.264 as it matches the encoding of my security system  The ultrafast preset and zerolatency tune optimize for real-time streamingThe Rick Astley FactorFor my testing purposes, I used Rick Astley’s “Never Gonna Give You Up” as my test footage. There’s something poetically fitting about Rickrolling your own security system!VLC showing RTSP feed of rick rollUsageTo use this container in your homelab:  Clone the repository containing the Dockerfile and entrypoint script    git clone https://github.com/TravisPooley/RTSP-Test-Server.gitcd RTSP-Test-Server        Create a videos directory and add your test video (or use Rick Astley for extra fun)  Build the Docker image:    docker build -t rtsp-server .        Run the container:    docker run -d --name rtsp-test -p 8554:8554 rtsp-server        Connect your security system to rtsp://your-docker-host:8554/mystreamBenefits and ApplicationsThis solution has several advantages:  Consistency: The same exact footage is used for every test  Efficiency: No need to physically trigger cameras  Integration Testing: Test your entire pipeline with known inputs  Isolation: Debug specific parts of your system without changing othersConclusionBuilding this RTSP Docker container saved me countless trips running in front of my security camera.Whether you’re building a sophisticated home security system or just experimenting with computer vision, having a reliable way to stream test footage can save you time and sanity.Feel free to adapt this solution to your needs, and remember: in the world of home automation testing, we’re never gonna give up making things better!Got a security camera system that needs testing? Check out the full code for this project on GitHub."
  },
  {
    "title": "Automated Trading: How 500 Lines of JavaScript Funded My Steam Library",
    "url": "/blog/steam-bot/",
    "categories": "Project",
    "tags": "JavaScript, Nodejs, Steam, Automation",
    "date": "2021-08-15 00:00:00 +0930",
    "snippet": "Steam Trading Card BotsIn the vast ecosystem of Steam, Valve’s digital distribution platform, a unique economy emerged around collectible trading cards. For years, I operated a Node.js trading bot ...",
    "content": "Steam Trading Card BotsIn the vast ecosystem of Steam, Valve’s digital distribution platform, a unique economy emerged around collectible trading cards. For years, I operated a Node.js trading bot that exchanged items like Team Fortress 2 keys for complete sets of these digital cards. What started as a weekend coding project eventually funded my entire Steam library. Here’s the story of how it worked and the fascinating ecosystem behind it.Understanding Steam Trading CardsSteam introduced trading cards in 2013 as collectible digital items that transformed the platform’s social experience. These virtual cards became the foundation of an entire ecosystem of collecting, trading, and crafting.How The System WorksEach supported game on Steam offered its own unique set of collectible cards:  Most games featured between 5-15 unique cards per complete set  Cards featured artwork from the game, making them visually appealing collectibles  Each card had varying rarity and market value based on the game’s popularityThe Collection ProcessThe genius of Valve’s system was that it encouraged social interaction and marketplace use:  Initial Acquisition: Players would automatically receive card drops while playing eligible games  Incomplete Sets: Most games would only drop about half of the full set to any single user  Social Component: The remaining cards needed to be acquired through trading with other players  Marketplace Option: Players could also buy specific cards from the Steam marketplace  Crafting Reward: Once a user collected a complete set, they could “craft” these cards into a game badgeThis deliberate design created natural scarcity that drove trading activity and marketplace transactions. Players couldn’t simply earn complete sets through gameplay alone, they needed to engage with the wider Steam community or marketplace to complete their collections.The Purpose: Profile Customisation and LevellingThe badges crafted via a completed trading card set formed the backbone of Steam’s profile levelling system. When users crafted a badge by combining a full set of trading cards, they would:  Earn 100 XP toward their Steam level  Unlock a random emoticon from the game  Receive a profile background from that game  Occasionally receive discount coupons for other Steam gamesLevelling up on Steam unlocked meaningful benefits, including:  One new profile showcase every 10 levels, letting users highlight rare achievements, favourite games, artwork, or custom collections  Increased friend list capacity, starting at 250 friends and growing by +5 friends per level  Greater visibility in community hubs and higher ranking on friends lists  Higher Steam levels also increased your chances of receiving “booster packs” (free random card drops)For many players, levelling up became a way to personalise their profile, showcase their rare items, and stand out in the Steam community - turning card collecting and badge crafting into a massive trading and crafting economy. Over time, a high Steam level became more than just a functional advantage; it became a status symbol. Really, it was all about the flex - showing off your dedication, collection, and time invested to everyone who visited your profile.What Are Steam Trading Card Bots?Steam trading card bots are automated accounts programmed to facilitate trades between users and the bot itself. These bots served as virtual marketplaces where players could exchange valuable virtual items, typically TF2 keys, CS:GO skins, or other commodities, for complete sets of Steam trading cards.The core functionality was surprisingly simple: users would initiate a trade with the bot, offer a predetermined “currency” (often TF2 keys), and in return, the bot would automatically provide complete sets of trading cards from various games.The Economics That Made It WorkThe economics behind this trading system created a perfect arbitrage opportunity. Here’s how the numbers broke down:The Value Chain  Currency: TF2 keys maintained a stable value of approximately ~$3.65 AUD each  Product: Complete card sets individually cost $0.30-0.70 on the Steam marketplace  Wholesale: My bot purchased bulk cards from farmers at wholesale prices  Retail: The bot would then sell those complete sets at 15-30 sets per TF2 keyWhy It WorkedUsers seeking to level up their Steam accounts found this system incredibly cost-effective. Rather than:  Spending $0.30-0.70 × 15 sets = $4.50-10.50 buying individual sets on the marketplace  They could trade a single ~$3.65 TF2 key for 15+ complete sets through my botThis pricing strategy created a win-win scenario:  For users: Getting sets at 40-75% below market price  For me: Creating profit through volume and arbitrage between wholesale and retail pricesThe margin between my bulk acquisition cost and what users would pay in keys created a sustainable profit cycle. Over time, this simple arbitrage bot generated enough virtual currency that I was able to withdraw and sell the keys, effectively funding my entire Steam library without spending additional money out of pocket.The Card Farming ConnectionThese trading bots didn’t exist in isolation, they were part of a larger ecosystem that included “card farmers.” Card farming referred to the practice of obtaining Steam trading cards with minimal effort, often through automated processes.When a Steam user plays a game, they’re eligible to receive a limited number of card drops (typically half of a game’s set). Once received, these cards could be traded or sold on the Steam marketplace.This led to the rise of specialised farming accounts, many operated out of regions where the value proposition made economic sense (Russia was particularly known for this). These farmers would:  Purchase extremely cheap games via back channels or games that are free for a limited time  Use specialised software like “Idle Master” or “ASF (ArchiSteamFarm)” to simulate playing these games to receive card drops  Collect the cards and either sell them directly on the marketplace or more often than not trade them in bulk to bots like mineThe most efficient operations would run thousands of accounts simultaneously, creating a constant stream of new trading cards entering the economy every time a new game became available to the farming community.How It All StartedI used one of these bots for the first time on 15 April 2017. I used it to get to level 50 on Steam, and then thought I wanted my own bot. Five days later, I traded the excess sets to my bot account.I’ve always had a natural curiosity when discovering new systems like this. There’s something captivating about understanding the mechanisms behind them, figuring out how they operate, and finding ways to optimise or improve them. This particular curiosity led me down a rabbit hole that would consume me for years.On 7 February 2018, I purchased a Chroma Case Key to test trading. I manually traded 5 times. This validated that you could make some profit without being one of the big players. At this point, I started to look into how to make the bot.Years ago, I had made what was effectively a Steam RAT (remote access trojan) in C#. I took the Steam API and used the chat hooks to trigger random functionality, inspired by the YouTube video Programming Drunk PC prank application in C# .NET : #Codegasm 4 - @Barnacules. My little RAT would shake the mouse, close their current application, and so on. So I was familiar with Steam bots, but the libraries I used for the RAT wouldn’t work since they had no trading functionality.From my research, it appeared that all of the big bots were using DoctorMcKay’s Steam libraries in NodeJS. I started working it out…By early March, I was ready to start experimenting with real items. I purchased an additional 7 keys and ran my first sets of automated trades in mid-March 2018. Testing continued throughout March until 29 March 2018, when I purchased another 2 keys to build stock for the soft launch. This meant when it launched, it had 10 keys worth of stock. After the soft launch, my trade logs became filled with me withdrawing keys, buying sets, and depositing the sets, as suppliers were hard to come by at the start.In late 2018, I added an additional 28 keys that I received for doing various freelance jobs for other bot owners. This primarily involved adding functionality from my bot to other people’s bots or creating custom scripts for them. Progress up until this point was really slow, but this is when the snowball effect started taking place.My SetupWhen I finally got the code set just right, I started optimising the setup. I ran 2 bots simultaneously, hosted on a Raspberry Pi so I could run them 24/7 with minimal cost.Raspberry Pi Running Both BotsIn reality, I probably didn’t need two bots given the volume I was experiencing, but it allowed me to take overstock on one without having larger farmers clear all of my inventory of keys without getting sufficient variety in sets.How Users Use the BotThe user experience was designed to be as simple and familiar as possible. I studied how the established bots operated and replicated their command structure to ensure users wouldn’t need to learn a new system. The interaction was entirely through Steam’s built-in chat functionality.When users wanted to make a trade, they would:  Add the bot as a friend on Steam (if they hadn’t already)  Send a chat message with their desired purchase using simple commands  Receive an automated trade offer within seconds  Accept the trade to complete the transactionThe command structure was intentionally straightforward, users would type commands like:  buy 1 - Purchase x sets using 1 CS:GO key (the default currency)  buytf 1 - Purchase x sets using 1 TF2 key  buyhydra 1 - Purchase x sets using 1 CS:GO Hydra Key (removed from the generic buy due to the fall in value of Hydra Keys)  buygems 1 - Purchase 1 sets using x Steam GemsFor whatever reason, CS:GO keys had become the default currency in the broader bot ecosystem, so buy without any suffix defaulted to CS:GO keys. All other currencies required their specific suffix to be added to the command. All x would be replaced by a defined exchange rate which will be dicussed in the next paragraph.Bot Message Commands For TradingBehind the scenes, each command triggered a series of validation checks implemented through a cascade of if statements:  Rate limiting: Has this user been making too many requests recently?  Input validation: Is the number legitimate? (Not a string, not absurdly high, not zero or negative, ect)  Trade conflict checking: Does the user already have pending trades with the bot?  Inventory verification: Do I have enough sets in stock to fulfill this order?  Payment verification: Does the user have enough of the specified currency in their inventory?If all checks passed, the bot would automatically generate and send a trade offer. If any check failed, the bot would respond with a helpful error message explaining what went wrong and how to fix it. This automated validation system prevented most issues before they could occur, keeping the trading process smooth and eliminating the need for manual intervention.Day-to-Day Operation of the BotRunning the bot day-to-day was surprisingly hands-off, requiring minimal intervention once it was up and running. The bot’s operations were mostly monitored out of curiosity rather than necessity. However, it was always fascinating to see the constant stream of activity-trade logs, messages, and updates coming in at all hours of the day, proving that the bot was consistently humming away.Bot backend trade logs &amp; bot messages alerting of new salesEach trade made by the bot was logged into its dedicated directory. For every day, a new text file was created with the trade details, including the time of the trade, the Steam ID of the user, and the item details. The trade logs were appended daily, making it easy to track the bot’s activities over time. On the flip side, there were also directories for chat logs, which captured the bot’s communication and provided insight into the flow of interactions.What was really cool was witnessing the bot’s ability to make a few cents to a few dollars on every trade, all without manual input. While the bot operated automatically, receiving messages about sales and trades was an exciting reminder that the system was functioning smoothly. However, in hindsight, I wish I’d kept more detailed records of the bot’s inventory. A weekly or monthly summary, for example, would have been great to track the exact amount of items or the total number of trades completed.{\t\"allowBuy\":true,\t\"pricecs\":20,\t\"allowBuyTf\":true,\t\"pricetf\":20,\t\"allowBuyHydra\":true,\t\"pricehydra\":5,\t\"allowBuyGems\":true,\t\"pricegems\":400,\t\"allowSell\":true,\t\"pricesell\":21,\t\"allowSellGems\":true,\t\"sellgems\":375,\t\"allowSellTf\":true,\t\"priceselltf\":21,\t\"allowSellHydra\":true,\t\"pricesellHydra\":11,\t\"maxStock\":30,\t\"maxKeys\":50,\t\"maxBuy\":100,\t\"maxSell\":50,\t\"maxLevelCheck\":999,}The real challenge came with items that had unstable market values. In the above example, certain items like Hydra keys could fluctuate significantly in price. When these price swings occurred, the bot’s selling rate would sometimes need to be adjusted to reflect the new market conditions. If the rates weren’t updated regularly, the bot risked clearing out its stock and holding items that were worth less than their original value. Fortunately, in the long run, most of these items appreciated in value, so any misses with pricing caused minimal damage.Bot StatisticsThe following stats aren’t lifetime numbers - they’re based on the best continuous archive I could find, covering the period from 26 February 2019 to 22 May 2021 (2,824 of the 4,167 total trades). Over time I changed the way the bot recorded and formatted its data, but this archive offers a really solid, normalised snapshot of the bot’s performance during those years.During this window, the bot sold 57,101 sets across all currencies. TF2 keys were by far the most common payment method, with 2,761 TF2 keys received for 48,892 sets. CS:GO keys were much rarer, with only 150 CS:GO keys used to buy 3,519 sets - likely a reflection of their limited supply compared to the incredibly stable and accessible nature of TF2 keys. On top of that, the bot received 1,885,647 gems in exchange for 4,690 sets.Across this period, the bot traded with 582 unique users. Just 10 users (less than 2% of the total), were responsible for 23% of all set purchases, buying 13,362 sets between them. The largest individual buyer snapped up 2,885 sets, paying with 164 TF2 keys, and the rest of the top 10 consistently purchased hundreds to over a thousand sets each. These users were true whales and a major source of volume for the bot. Looking at the profiles of these users, it appears most of them were also selling sets, so it’s likely they were using my bots to get inventory for their service. This is fine though I still made the same margin no matter who bought my sets.Cashing OutI couldn’t find a way to export how much I had pulled out of the bot, but I could see my market history from my main account. Again, I couldn’t find a way to export this data, so I reverted to my old trick of when a website doesn’t provide the functionality I want: I hack their DOM. I wrote a quick and dirty DOM script to export all of the market pages one by one.After analysing this data and removing the irrelevant sales, I found that:  A very rough approximate would put items used by bot at ~300$ (primarily spent on inventory expansions and levels on the bot for more friend slots)  In total I withdrew just over $1,150 worth of items from the bot  So far I’ve spent a little over $450 of items  At the moment I’ve got 175 TF2 keys waiting for a rainy day to be soldThis process was rather slow due to the low amount of capital injected, the snowball effect of having more stock would have been huge. Steam instituted a trade lock on items such as keys for a week after each trade. This meant after every sale my bot made at the start, I would be stuck for a whole week. Over time as I got more stock, this was less of a limiting factor and I was more held back by the limited number of suppliers I had at my desired price. If I had started with a higher initial investment, the total return would likely have been far higher.ConclusionOver the 3 years, 5 months, and 14 days the bot ran, I started with an initial investment of $130 and ended up withdrawing $1,150 worth of items, which meant a total profit of around $1,020. That works out to an average profit of about $0.24 per trade, based on the 4,167 trades made. If you break it down further, that’s an average profit of about $0.28 per day when extrapolated over the entire period.This means I achieved a return of around 785%. To put that into perspective, the annualised return, or compound annual growth rate (CAGR), works out to approximately 84.5% per year. That means if I had reinvested the profits each year, the bot would have been growing at nearly nine times the rate of a typical long-term stock market return.If you compare the $0.28 per day passive income to another passive income method like dividends, it’s a pretty good way of leveraging my money, and even better yet, I learnt a language out of it. JavaScript has become my go-to language for day-to-day programming, and it’s immensely useful as seen by all of the DOM hacking I do.It is important to note that these kinds of returns aren’t likely to be sustainable today. I ran the bot at what might have been the peak of the Steam set market, when there was more opportunity and less competition. Nowadays, returns are much smaller due to more restrictions and increased market saturation."
  },
  {
    "title": "Automated Luck: How My Twitter Bot Won Contests While I Slept",
    "url": "/blog/twitter/",
    "categories": "Project",
    "tags": "Python, Automation",
    "date": "2018-06-15 12:00:00 +0930",
    "snippet": "The InspirationA few years ago, I stumbled upon Hunter Scott’s fascinating DEF CON 24 talk titled “RT to Win: 50 Lines of Python Made Me the Luckiest Guy on Twitter”. The concept was brilliantly si...",
    "content": "The InspirationA few years ago, I stumbled upon Hunter Scott’s fascinating DEF CON 24 talk titled “RT to Win: 50 Lines of Python Made Me the Luckiest Guy on Twitter”. The concept was brilliantly simple yet remarkably effective: create a bot that automatically finds and enters Twitter giveaways.Scott’s experiment ran for about 9 months, entered approximately 165,000 contests, and won nearly 1,000 prizes. His haul included everything from concert tickets to a trip to New York Fashion Week, and even unusual items like a cowboy hat signed by the cast of a Mexican soap opera.Intrigued by both the technical simplicity and potential rewards, I decided to build my own version.Learning Through BuildingWhat makes this project even more significant for me personally is that before starting this project I had zero experience with Python, web scraping, or databases. I intentionally chose this project as a vehicle to learn these skills with a clear, motivating goal in mind.This approach of learning by building real projects with concrete outcomes has been my go-to method for acquiring new programming skills. By tackling projects that genuinely interest me, I’ve successfully learned a dozen different programming languages and technologies over the years.There’s something uniquely effective about having a tangible end goal that keeps you pushing through the inevitable challenges and roadblocks. When you’re excited about what you’re building, debugging becomes a puzzle to solve rather than a chore, and each new concept has immediate practical application.The contest bot provided perfect scaffolding for learning:  Python fundamentals through practical application  API integration via Twitter’s interface  Database design and management  Web development through the PHP dashboardThis project-first approach to learning accelerated my skills acquisition far beyond what traditional tutorials or courses could have accomplished in the same timeframe.Open Source (With Strategic Limitations)For those interested in creating their own contest bot, I’ve made a version of my code available on GitHub. However, I should note a few important caveats:This was my first time using Python and my first foray into functional programming, so the code is… let’s say “less than ideal.” It serves as a genuine chronicle of my learning process, complete with all the quirks and inefficiencies of a beginner.The version I uploaded isn’t actually my most sophisticated iteration. As Twitter began cracking down on bots, I became less concerned about other people starting competing bots and more focused on maintaining my own statistical advantage. Many of the features mentioned in this article as either implemented or roadmap items were actually built into more advanced versions that I never made public.Consider the GitHub repository a foundation to build upon rather than a polished product. The real value came from the iterations and improvements I made over time as I learned more about Python, Twitter’s API, and bot detection mechanisms.By keeping my most effective techniques private, I maintained the statistical edge that made this project so successful. After all, the fewer contest bots in operation, the better my chances of winning remained!How It WorksThe functionality is straightforward:  Search Twitter for contest-related keywords like “retweet to win,” “RT to win,” “contest,” “giveaway”  Filter results to only include tweets that can be entered on the Twitter platform (request retweets for entry)  Follow accounts when required as part of the contest rules  Automatically retweet qualifying tweets  Track entries and wins for data analysis  Manage follows/unfollows to stay within Twitter’s limits  Repeat the process at regular intervalsI also implemented some additional features based on Scotts bot. To prevent getting banned by Twitter, I had to carefully manage rate limits. Twitter restricts how often you can tweet, retweet, and follow/unfollow accounts. Since most contests required following the original poster, I implemented a FIFO (First In, First Out) queue to ensure I was only following the 2,000 most recent contest entries (staying well within Twitter’s following limits).Reaching The LimitsWorking with Tweepy v3.6.0, I quickly discovered how Twitter’s API rate limits became the central challenge for my giveaway bot. These limits were surprisingly restrictive, approximately 300 retweets per day, 1,000 likes per day and 400 follows per day. For searching arguably the most important functio of my bot, I was constrained to just 180 search requests per 15-minute window.Let’s break down what this meant for my bot’s theoretical maximum capacity: Each search could return up to 180 tweets, letting me scan about ~15,480 tweets daily. However, after filtering for actual giveaways (roughly 5% of tweets contained legitimate contests), I could potentially find about 774 giveaway opportunities daily. Unfortunately, I couldn’t enter them all due to the action limits. The retweet limit of 300 per day became my primary bottleneck, essentially capping my daily entries regardless of how many more contests I discovered.Given these constraints I optimised what I could, the search API was my most significant constraint that I had control over, like Scott I pivoted to using BeautifulSoup for web scraping Twitter’s interface directly. This approach completely bypassed the search API limits, allowing me to discover substantially more giveaways without hitting the rate limits that restricted my searches. The bot could now crawl Twitter’s web interface continuously, finding thousands of additional contest opportunities that would have been missed using the API alone. This innovation shifted my bottleneck entirely from discovery to action limits (retweets, likes, follows).The real breakthrough came when I moved away from basic searching functions to a more sophisticated architecture that saved discovered contests into a central SQLITE database. This allowed multiple bot accounts to share the same pool of discovered giveaways, effectively multiplying my entry capacity without increasing the computational overhead of search operations. Rather than each account wastefully duplicating the same search queries, one efficient scraping process could feed contest opportunities to an entire network of bots.Just using two accounts would theoretically double the number of entries I could perform daily, from 300 to 600 retweets. By the end of the project, I had over half a dozen bots operating at any given time, each capable of the full 300 daily retweets whilst drawing from the same centralised contest database.Finding Hidden Gems: The Statistical EdgeUnlike Scott, who believes he reached a scale where he was entering virtually every Twitter contest available, my operation was more modest. However, this led to an interesting discovery: my bot was finding giveaways that seemingly no one else was discovering.This created a significant statistical advantage. When a poorly advertised giveaway had only a handful public entries, and I had multiple bot accounts all running the same search queries, my chances of winning skyrocketed. This scenario played out numerous times. For example one brand, was giving away iTunes gift cards every month. Across my various accounts, I won 5 of their monthly giveaways because so few people even knew they existed.The Itunes giveaway that my bot won consistently (my accounts censored in green other accounts that won in red)The majority of the items I won were obtained through statistical means, such as entering contests multiple times and discovering giveaways that were not attracting large crowds. I essentially discovered a sweet spot.That’s not to say I only won the obscure contests. One of my most memorable wins was a copy of the game Rust in a giveaway where my odds of winning were under 2%. This was one of those cases where I would have entered manually if I had known about it, and I was thrilled when my bot managed to beat the odds.The announcement of the winner of the rust giveawayBlending InAnother way my bot differed, was unlike Scott’s approach of using generic Windows stock photos and minimal profile details (the username “HS” with the description “I’m up all night to get lucky”), I discovered that creating memorable, meme-based personas significantly improved my bot’s perceived legitimacy.My primary bot account was named “The_L3gend27”, a leet-speak version of the popular mobile gaming meme from 2016 TheLegend27 (the original TheLegend27 username was already taken). This reference to a well-known meme gave my account an immediate cultural context that appeared more authentic than generic or randomly generated names.I took advantage of one of the most common prizes I won (free graphic design work) to further enhance my profile’s legitimacy. When artists offered banners or profile graphics as giveaway prizes, I would claim them and immediately implement them on my bot accounts. This created a positive feedback loop: winning design giveaways provided me with professional-looking profile elements, which in turn made my accounts look more legitimate for future contests.Profile of my primary bot, with banner art won by the botThis strategy paid dividends in unexpected ways. Several contest hosts specifically mentioned recognising my distinctive username when announcing me as a winner. For manually selected giveaways (where hosts weren’t using random selection tools), having a memorable, recognizable name likely increased my chances significantly. The lengthy, distinctive “The_L3gend27” was more likely to catch a host’s eye than forgettable generic usernames.Host of a giveaway making reference to The Legend27 memeWhat Qualified as a GiveawayOne of the most critical challenges in optimising my Twitter contest bot was accurately identifying legitimate giveaways while filtering out false positives. With Twitter’s strict rate limits constraining me to just 300 retweets per day, 1,000 likes per day, and 400 follows per day, every action had to count towards genuine contest opportunities.I developed a multi-layered filtering system based on pattern recognition and probabilistic scoring. Each discovered tweet received a “giveaway probability score” calculated from multiple weighted signals that determined whether it warranted one of my precious daily actions.The first layer involved sophisticated keyword filtering that went far beyond simplistic string matching. Many tweets contained phrases like “I just retweeted John Smith’s post I hope I win”, which would trigger basic bots despite not being actual contests. My system required multiple giveaway-specific keywords to appear in logical relationships to each other, such as “retweet AND win” or “RT AND giveaway”, with additional context clues like prize descriptions or entry deadlines.Most critically, I implemented social action requirements as a key qualifying factor. Since the vast majority of legitimate Twitter giveaways aimed to increase follower counts and engagement metrics, I prioritised contests that explicitly requested following the host account alongside retweets or likes. This single filter dramatically improved my success rate by eliminating noise while focusing on contests where my entry would be properly tracked by the host.The filtering strategy inevitably meant missing some valid giveaways that didn’t match my strict criteria. However, given Twitter’s fundamental rate limit constraints, attempting to enter every possible contest was mathematically impossible. The platform’s restrictions forced a strategic choice: scatter limited actions across uncertain prospects, or concentrate them on high-probability targets.Quality over quantity proved to be the optimal approach. Rather than wasting precious daily actions on ambiguous tweets, I focused my constrained resources on contests with the highest legitimacy probability and prize. This targeted strategy maximised both my win rate and the overall value of prizes claimed.Automating the CollectionWinning dozens of digital prizes weekly created an unexpected problem: claiming all these rewards became a significant time sink. Interacting with contest hosts, providing platform information, or submitting game details manually for each win quickly became unsustainable.I developed a solution born from necessity: automated prize collection. My bot monitored direct messages for specific patterns and keywords that indicated I had won something, then automatically responded with appropriate information. For digital prizes (which made up most of my winnings), this system was particularly effective. When the bot detected messages containing phrases like “trade link”, it would automatically reply with pre-formatted responses containing my relevant platform information.Another critical reason for implementing this automation was the time-sensitive nature of many giveaways. Numerous contest hosts would give winners only 30 minutes to respond before redrawing. Living in Australia meant I was often asleep when these messages arrived. A 3 AM notification meant I’d frequently miss out on prizes I’d legitimately won. The automated response system solved this problem entirely, ensuring I could claim prizes regardless of time zone differences.This automation meant that digital prizes like game keys, in-game items, and account credentials flowed in without any manual intervention,  I’d simply check my accounts periodically to find new items waiting.However, the system wasn’t flawless. Occasionally, users would inadvertently trigger the automated responses during normal conversations, leading to some genuinely confusing moments. Someone might casually mention a “trade link” or use other trigger phrases in context, only to receive an unexpected automated response containing my Steam trade link. These incidents were typically brushed off after a confused “huh?” response, and most people seemed to either ignore the oddity. Fortunately, I never encountered anyone who discovered the trigger phrases and deliberately exploited them repeatedly, a very real risk that could have made the bot’s automated nature embarrassingly obvious to the broader Twitter community.Combating the Bot SpottersAnother fascinating discovery during this project was the existence of “bot spotters”. These are accounts specifically designed to catch and expose contest bots like mine. These accounts, often named something like “bot_spotter” or using variations with leet speak (b0t_sp0tt3r), would cleverly tweet contest-like phrases containing all the keywords our bots look for.An early itteration of my bot falling for bot spotting accountsTo counteract this, I implemented several protective measures:I developed a multi-pronged detection system that monitored accounts for suspicious patterns. The first layer involved scanning usernames for variations of “bot,” “spotter,” “sp0tter,” and similar terms that these accounts typically used. However, username filtering alone wasn’t sufficient, as some bot spotters used completely innocuous names.The second layer tracked giveaway frequency patterns. My system maintained a database of how many “contests” each account had posted over various time periods. Any account exceeding predetermined thresholds such as posting more than five giveaways in a single day or twenty in a week, would trigger automatic blocking. Legitimate brands and influencers rarely posted contests with such frequency, making this an effective filter.When these thresholds were breached, the offending accounts entered a quarantine system. Rather than permanently blocking potentially legitimate accounts, suspicious users remained in quarantine until I could manually review and verify them. This prevented false positives while maintaining protection against bot spotters who might attempt to evade detection by changing usernames or tactics.This “arms race” between bots and bot spotters added an unexpected but fascinating dimension to the project!The Dark Side of Twitter ContestsRunning this bot revealed a troubling reality: a large portion of the contests I “won” had sinister surroundings.The most common scam I encountered involved “winners” being told they needed to pay small “delivery fees” to claim extravagant prizes like gaming consoles or smartphones. These scammers operated on a simple principle: the excitement of winning something valuable would cloud the winner’s judgment enough to overlook the red flag of paying a fee.These delivery fee scams were just the tip of the iceberg. Other common patterns included:  Contest hosts who never actually delivered prizes or responded to winners once they made contact  Accounts running perpetual contests to artificially inflate follower counts with no intention of ever giving anything away  Sophisticated phishing schemes disguised as contest verification, designed primarily to harvest personal information, passwords, or credit card details  Contests requiring excessive personal details for entry that were likely being compiled into marketing databases  “Follow loops” where multiple accounts would cross-promote each other’s fake giveaways to rapidly build their follower basePerhaps most concerning was the significant number of giveaways offering stolen goods as prizes. The most prevalent were stolen accounts for premium services like Netflix, Spotify, Minecraft, Steam, and other gaming platforms. These contests were essentially fencing operations disguised as generous giveaways.Becoming a Tweepy Expert and Giving BackAfter wrestling with the Tweepy library for many months to build and refine this project, I developed a deep understanding of Twitter’s API and the nuances of Tweepy. I encountered numerous challenges that had no clear solutions online, forcing me to dig deep into the documentation and experiment extensively.When searching for solutions to my own development challenges, I frequently encountered the same unanswered questions on Stack Overflow that had been plaguing other developers for months or even years. Remembering my own frustration when facing these roadblocks with no available solutions, I decided to become the kind of Stack Overflow contributor I wished I’d found during my development process.I focused particularly on Tweepy-related questions that mirrored the exact challenges I had wrestled with during development. These ranged from basic searching and filtering techniques to more complex problems around cursor pagination and error handling issues that had little to no clear documentation or working examples available online. My Stack Overflow contributions became a way of documenting the solutions I wished had existed when I was struggling through the same problems months earlier.Initially, I only responded to a few new questions, hoping those users would still be interested in a solution. The response was overwhelmingly positive, and I found myself enjoying the process of helping others overcome the same hurdles I had faced. This unexpected side effect of the project led to me climbing into the top percentage of Stack Overflow users for a period, which was a rewarding achievement in itself.The ResultsWhen I first launched my Twitter contest bot in January 2017, I was meticulous about tracking every single win, no matter how small. I quickly discovered that maintaining this list became a job in itself. After just over a month, I abandoned the formal tracking as the volume of wins (particularly low-value digital items) made it impractical to document everything.My One-Month HaulDuring that first month of careful record-keeping (January-February 2017), my bot managed to win 75 prizes in just over 30 days. That averages to about 2.5 wins per day - not quite as prolific as Hunter Scott’s 4 per day, but surprisingly effective considering how primitive my system was at this stage.It’s important to note that during this initial period, my bot was extremely inefficient. It was still struggling with filtering out non-contest tweets and was entirely limited to Twitter’s cursor search functions, which would return a maximum of 180 tweets per query. Even worse, many of these returned tweets were duplicates, severely reducing my theoretical maximum discovery rate. The filtering algorithm was also quite basic at this point, frequently wasting precious retweet actions on false positives that weren’t actual contests.Looking at the categories of items won during this tracked period (the complete list of wins provides the full details):  Digital accounts/subscriptions: 20 items (26.7%) - including Netflix, Spotify, Minecraft, and Crunchyroll accounts  In-game items/currency: 19 items (25.3%) - CS:GO skins, Pokémon codes, game server ranks  Digital services: 12 items (16%) - banners, GFX designs, 3D intros  Cash/gift cards: 5 items (6.7%) - ranging from $0.03 PayPal to $15 iTunes cards  Physical items: 19 items (25.3%) - including gaming hardware, clothes, collectables, and kitchenwearUnlike Scott, who tracked every win over his 9-month project and published a complete list of winnings, my tracking dropped off precisely because the volume became unmanageable alongside my other responsibilities. In hindsight, this was a missed opportunity for more comprehensive data analysis.I would also go through the DMs to see what I won, but unfortunately, during Twitter’s crackdown on bots, many accounts were nuked, including my bot accounts and even those of my adversaries (those pesky bot spotters). That’s made finding evidence and media for this post particularly difficult (hence all the light mode screenshots - I’m sorry). However, whilst the accounts have been suspended, if you know where to look you can still find other people’s announcements of me winning their competitions using Twitter advanced searches like this one.The Ethical AccountingIt’s worth noting that many of my “wins” were ethically questionable in retrospect. The digital accounts that comprised 30% of overall winnings were clearly compromised or stolen goods being redistributed through these contests. While I didn’t utilise these prizes, they represent a significant portion of the “value” generated by the bot.Discovery and ExposureIn retrospect, it should have been extremely obvious that my account was a bot. My accounts like “The_L3gend27” were doing nothing but entering giveaways 24/7 with no apparent sleep schedule, posting no original content, and never engaging in normal social interactions. Despite my attempts to create a memorable persona, the behavioral patterns were a dead giveaway to anyone paying attention.I did get called out a few times, mostly via direct messages, but occasionally publicly. These public call-outs were particularly damaging because they would cause me to lose giveaways I would have otherwise won. Contest hosts who saw comments like “this is obviously a bot account” would often disqualify me even after initially announcing me as a winner.Eaxmple of public accusations of bot behavior that cost me several prizesInterestingly, I also received messages from users who had figured out a clever strategy: they used my account as a giveaway discovery service. Rather than building their own filtering systems, they would simply monitor my retweets to find active contests. This was actually brilliant - they shifted all the computational overhead of finding and filtering giveaways onto my bot while simultaneously degrading my odds of winning by increasing competition. They got a curated feed of legitimate giveaways without any of the development work, while I unknowingly provided them with a competitive advantage against my own operation.ConclusionScott’s DEF CON talk demonstrated how a simple script could achieve impressive results through automation at scale. My implementation confirmed that the approach still worked years later, though Twitter’s landscape continued to evolve.The project perfectly illustrates how programming allows us to automate repetitive tasks, apply statistical advantages at scale, and occasionally win free stuff along the way. It also offers a fascinating glimpse into the ecosystem of online contests.Beyond the prizes, this project yielded unexpected benefits: it served as an effective vehicle for learning Python, web scraping, and database management from scratch. It reinforced my belief that project-based learning with clear goals is the fastest way to acquire new programming skills. The knowledge gained allowed me to contribute to the developer community and help others facing similar challenges.If you’re looking to learn a new programming language or technology, I strongly recommend finding a project that genuinely interests you and diving in. The motivation of building something real will carry you through the learning curve far more effectively than abstract tutorials ever could.Note: If you plan to create your own contest bot, be sure to review Twitter’s current Developer Policy and Terms of Service to ensure compliance.PS If you also ran or are running a Twitter giveaway bot, I’d love to hear about your experiences. Use the contact link to share your story!"
  }
]
